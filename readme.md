## Instructions for running

I used google colab (python 3) to perform all training, since I don't have a GPU available to me. The easiest way to run the training would be to create a folder named `DeepStep_colab` in the root of your google drive. Then, copy the necessary Jupyter notebooks into this folder:
- `PreTraining/DeepStep_PreTrain.ipynb` (generates `InceptionV3_spectrogram_pretrained_weights.hdf5` as well as aditional log and .hdf5 files, which are not used by other scripts)
- `DeepStep_bottleneck_feature_gen.ipynb` (generates `deepStep_bottleNeck_data.npz`)
- `HyperOpt/DeepStep_hyperopt.ipynb` 
- `FinalTrain/DeepStep_finaltrain.ipynb` (generates the .hdf5 file of the final trained model)

In adittion, save a copy of the **[zip file (deepStep_data_spect.zip)](https://drive.google.com/open?id=1bnaGOYfbU7KD6jsE5w3N_7Uk9SC0Oh7w)** containing the spectrogram data into this folder. After this, you should be able to run the Jupyter notebook in google colaboratory in the above order, and they'll generate all necessary further files.

If you intend to run the training on a local machine, you'll have to modify the file paths in the notebooks accordingly.

Please note that the hyperparameter optimalization requires `tensorflow 2` and `keras-tuner` to be installed. The other notebook use the standalone `keras` package.

In addition to these necessary files, the data generated by them can also be found in this repo, in the case they are not too large for github.

The `.hdf5` file for the final trained model is available **[from here](https://drive.google.com/open?id=1-3LrzIF40AZzIEqjc7_K_xQl61_ZIkiX)**.

The process for processing the raw recording is performed using the `wav_process.py` python script. This I performed on the local machine. Have the python script and the `data_raw` folder from [this archive](https://drive.google.com/open?id=1scHsJlCOq0luO0JiUdKg4VH2WY5fYLa1) in the same folder and run the script. Make sure that the sript has privilages to create new folders, or else you'll have to create the appropiate folder structure beforehand, the same way as it is in 'deepStep_data_spect.zip'.

The documentation for the project can be found in the `doc` folder.

## Data collection

The data used in this project was collected by recording the sound of footsteps of participants. In total it contains 33 different person - shoe combinations among 6 persons, with 20 recordings for each combination. The basic data for the participants can be found in the `data.csv` file.

** The data for this project can be found [here](https://drive.google.com/open?id=1scHsJlCOq0luO0JiUdKg4VH2WY5fYLa1) for now.**

Raw recordings can be found in the `data_raw` folder. Each file contains the footsteps recorded by two microphones: a *Shure SM-58* dynamic microphone and an *AudioTechnica AT2020* condenser microphone, as the left and right channels of the recording respectively. All recordings are 32 bit float @ 44.1 kHz. Also included in the raw recordings a noise sample of the setup: a recording made with nothing making sound actively.

Processed recordings can be found in the `data` folder, organised by recording microphone first, and *train/vaildate/test* split on the next level.

The process for processing the raw recording is performed using the `wav_process.py` python script. The outline of the algorithm is as follows:
1. Reading the noise sample and splitting it by channel
2. Applying a 2nd order highpass butterworth filter to both noise samples (cutoff frequency = 1 kHz)
 I have found that there are some significant artifacts after the noise reduction step if using the unfiltered noise sample.
3. Iterating through each recording:
	a. Splitting the channels
	b. Applying the noisereduction using the noise samples
	c. Applying a higpass filter  (cutoff frequency = 1 kHz)
	d. Applying a notch filter between 43 and 47 Hz
	e. Create a mel spectrogram from the sound wave to use in a CNN

## Proof of Concept training

Work done for the second milestone:

The processed spectrograms ordered in a file structure suitable for transfer learning. The appropriate zip file is located **[here](https://drive.google.com/open?id=1bnaGOYfbU7KD6jsE5w3N_7Uk9SC0Oh7w)**.

I used InceptionV3 as a pretrained CNN. Although the weights for the imagenet training didn't yield very good results at first, and trying to pretrain only a top dense classifiication layer pretty much failed, I was able to get an **84%** test accuracy with retraining the whole InceptionV3 CNN. The training was performed in google collab, and it's process can be seen in `PoC/deepStep_transfer_PoC.ipynb` (this notebook is strongly based on my solution for the 4th small assignment). The weights for this CNN are located in `full_model.hdf5`, available from my drive: [full_model.hdf5](https://drive.google.com/open?id=1H5F1x7gXDTHbBxvSrYcN_rhQ4d5tL0Ae)
